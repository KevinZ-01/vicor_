<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="GLIGEN:Open-Set Grounded Text-to-Image Generation.">
  <meta name="keywords" content="Image Generation, Diffusion, Grounding">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>GLIGEN:Open-Set Grounded Text-to-Image Generation.</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="icon" href="./static/images/icon.png">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  </head>
  <body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">GLIGEN:</h1>
          <h2 class="title is-2 publication-title">Open-Set Grounded Text-to-Image Generation</h2>
          <div class="is-size-5">
            <span class="author-block">
                <a href="https://yuheng-li.github.io/" style="color:#f68946;font-weight:normal;">Yuheng Li</a>,                
            </span>
            <span class="author-block">
              <a href="https://hliu.cc/" style="color:#f68946;font-weight:normal;">Haotian Liu</a>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.ca/citations?user=HDiw-TsAAAAJ&hl=en/" style="color:#F2A900;font-weight:normal;">Qingyang Wu</a>,
            </span>
            <span class="author-block">
              <a href="https://pages.cs.wisc.edu/~fmu/" style="color:#f68946;font-weight:normal;">Fangzhou Mu</a>,
            </span>
            <span class="author-block">
              <a href="https://jwyang.github.io/" style="color:#008AD7;font-weight:normal;">Jianwei Yang</a>,
            </span>
            <span class="author-block">
              <a href="https://www.microsoft.com/en-us/research/people/jfgao/" style="color:#008AD7;font-weight:normal;">Jianfeng Gao</a>,
            </span>  
            <br>
            <span class="author-block">
              <a href="https://chunyuan.li/" style="color:#008AD7;font-weight:normal;">Chunyuan Li<sup>*</sup></a>,
            </span>   
            <span class="author-block">
              <a href="https://pages.cs.wisc.edu/~yongjaelee/" style="color:#f68946;font-weight:normal;">Yong Jae Lee<sup>*</sup></a>,
            </span>                                  
          </div>

          <br>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><b style="color:#f68946; font-weight:normal">&#x25B6 </b> University of Wisconsin-Madison; </b></span>
            <span class="author-block"><b style="color:#F2A900; font-weight:normal">&#x25B6 </b>Columbia University; </span>
            <span class="author-block"><b style="color:#008AD7; font-weight:normal">&#x25B6 </b>Microsoft</span>
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://yuheng-li.github.io/GLIGEN/" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://yuheng-li.github.io/GLIGEN/" target="_blank" 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (coming soon)</span>
                  </a>
              </span>

              <span class="link-block">
                <a href="https://aka.ms/gligen"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Demo</span>
                  </a>
              </span>
              <!-- <span class="link-block">
                <a href="https://youtu.be/wYp6vmyolqE"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                  </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" width="120%" src="images/teaser_v4.png">
      <h2 class="subtitle has-text-centered">
        <p style="font-family:Times New Roman"><b>Figure 1. GLIGEN enables versatile grounding capabilities for a frozen text-to-image generation model.</b></p>
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large-scale text-to-image diffusion models have made
            amazing advances. However, the status quo is to use
            text input alone, which can impede controllability. In this
            work, we propose GLIGEN, <b>G</b>rounded-<b>L</b>anguage-to-<b>I</b>mage
            <b>Gen</b>eration, a novel approach that builds upon and extends
            the functionality of existing pre-trained text-to-image diffusion models 
            by enabling them to also be conditioned on
            grounding inputs. <b>To preserve the vast concept knowledge
              of the pre-trained model, we freeze all of its weights and
              inject the grounding information into new trainable layers
              via a gated mechanism</b>. Our model achieves open-world
            grounded text2img generation with caption and bounding
            box condition inputs, and the grounding ability generalizes
            well to novel spatial configuration and concepts. GLIGENâ€™s
            zero-shot performance on COCO and LVIS outperforms that
            of existing supervised layout-to-image baselines by a large
            margin.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    <br>
    <br>






    <section class="section">
      <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Demo Visualization</h2>
            <div class="column is-full-width">
              <div class="columns is-centered">
                <img id="teaser" width="80%" src="images/same_box.gif">
              </div>
              <div class="columns is-centered">
              <!-- <h1>
                <p style="font-family:Times New Roman"><b>Demo. Left: Green Apple -> Rabbit, Darkest Red Apple -> Red Pear, Table -> Plate. Middle: Dog Removal, Right: Sky -> Mountain</b>
              </h1>                  -->
            </div>
          
            </div>
          </div>
        </div>
        <!--/ Abstract. -->


        <br>
        <br>

        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Demo Instruction</h2>
            <div class="publication-video">
              <iframe src="https://user-images.githubusercontent.com/6631389/211468127-60dd5c69-8db9-43d2-a45d-e97b336e5249.mp4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
            </div>
          </div>
        </div>

        <br>
        <br>

        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">
            <h2 class="title is-3">Model</h2>
            <div class="content has-text-justified">
              <p>
                <b>Key Designs for GLIGEN</b>:
              </p>
              <ul>
                <li>GLIGEN builds upon existing pretrained diffusion models and <b>freeze</b> original weights to preserve vast knowledge.</li>
                <li>A new trainable Gated Self-Attention layer absorbing new grounding input is added at each transformer block.</li>
                <li>Grounding tokens incorporates two information: <b>semantic</b> of grounded entity (encoded text or image) and <b>spatial location</b> (encoded bounding box or keypoints).</li>
              </ul>
            </div>        
            <img id="model" width="50%" src="images/approach_v1.png">
            <h3 class="subtitle has-text-centered">
              <p style="font-family:Times New Roman"><b>Figure 2. Gated Self-Attention is used to fuse new grounding tokens.</b></p>
            </h3>   
            <br>
            <br>
            <div class="content has-text-justified">
              <p>
                <b>Modulated Design</b>:
              </p>
              <ul>
                Compared with other ways using a pretrained diffusion model such as whole model finetuning, our approach is modulated and more storage-friendly. Just like Lego, one can plug and play different trained layers to achieve different functionalities.

              </ul>                    
            </div>          
            <img id="model" width="80%" src="images/approach_change.gif">
            <!-- <h3 class="subtitle has-text-centered">
              <p style="font-family:Times New Roman"><b>Figure 3. Unify different types of vision and vision-language tasks with a single X-Decoder.</b></p>
            </h3>         -->

            <br>
            <br>
            <div class="content has-text-justified">
              <p>
                <b>Scheduled Sampling</b>:
              </p>
              <ul>
                As a pretrained diffusion model has rich semantics and high visual quality and training cost is high, GLIGEN supports scheduled sampling in which the model can dynamically choose to use grounding tokens (by adding the new layer) or original diffusion model with good prior (by kicking out the new layer) to balance generation quality and grounding ability.
              </ul>                    
            </div>          
            <img id="model" width="80%" src="images/approach_sampling.gif">
            <!-- <h3 class="subtitle has-text-centered">
              <p style="font-family:Times New Roman"><b>Figure 3. Unify different types of vision and vision-language tasks with a single X-Decoder.</b></p>
            </h3>         -->
            </div>
          </div>
    </div>
  
    <!--/ Paper video. -->          
  </div>
</section>


<section class="section">
  <!-- Results. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3">Results</h2>
    </div>
  </div>
  <!-- </div> -->
  <!--/ Results. -->    
<div class="container is-max-desktop">


  <!-- Grounedtext2img. -->
  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-4">Text Grounded T2I Generation (Bounding box)</h2>
      <img id="teaser" width="95%" src="images/groundtext2img.png">
      <h1>
        <p style="font-family:Times New Roman"><b>By exploiting knowledge of pretrained text2img model, GLIGEN can generate varieties of objects in given locations, it also supports varies of styles.</b>
      </h1>                 
    </div>
  </div>

  <br>
  <br>
  <br>

  <!-- counterfactual. -->
  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-4">Spatially counterfactual generation</h2>
      <img id="teaser" width="95%" src="images/counterfactual.png">
      <h1>
        <p style="font-family:Times New Roman"><b>By explicitly specifying object size and location, GLIGEN can generate spatially counterfactual results which are difficult to release through text2img model (e.g., Stable Diffusion).</b>
      </h1>                 
    </div>
  </div>


  <br>
  <br>
  <br>

  <!-- counterfactual. -->
  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-4">Image Grounded T2I Generation (Bounding box)</h2>
      <img id="teaser" width="95%" src="images/image_condition.png">
      <h1>
        <p style="font-family:Times New Roman"><b>GLIGEN can also ground on reference images. Top row indicates reference images can provide more fine-grained details beyond text description such as style and shape or car. The second row shows reference image can also be used as style image in which case we find ground it into corner or edge of an image is sufficient.</b>
      </h1>                 
    </div>
  </div>


  <br>
  <br>
  <br>

  <!-- counterfactual. -->
  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-4">Grounded T2I Generation (Keypoints)</h2>
      <img id="teaser" width="95%" src="images/keypoint.png">
      <h1>
        <p style="font-family:Times New Roman"><b>GLIGEN can also ground human keypoints while doing text-to-image generation.</b>
      </h1>                 
    </div>
  </div>


  <br>
  <br>
  <br>

  <!-- counterfactual. -->
  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-4">Grounded Inpainting</h2>
      <img id="teaser" width="95%" src="images/inpaint.png">
      <h1>
        <p style="font-family:Times New Roman"><b>Like other diffusion models, GLIGEN can also perform grounded image inpaint, which can generate objects tightly following provided bounding boxes.</b>
      </h1>                 
    </div>
  </div>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
@article{zou2022xdecoder,
  author      = {Zou, Xueyan and Dou, Zi-Yi and Yang, Jianwei and Gan, Zhe and Li, Linjie and Li, Chunyuan and Dai, Xiyang and Wang, Jianfeng and Yuan, Lu and Peng, Nanyun and Wang, Lijuan and Lee, Yong Jae and Gao, Jianfeng},
  title       = {Generalized Decoding for Pixel, Image and Language},
  publisher   = {arXiv:2212.11270v1},
  year        = {2022},
}
</code></pre>
  </div>
</section>

<section class="section" id="Acknowledgement">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgement</h2>
    <p>
      This website is adapted from <a
      href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> and <a
      href="https://x-decoder-vl.github.io">X-Decoder</a>, licensed under a <a rel="license"
                                          href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
      Commons Attribution-ShareAlike 4.0 International License</a>.
    </p>
  </div>
</section>


</body>
</html>
